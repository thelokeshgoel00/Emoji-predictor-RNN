{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2811"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emoji.EMOJI_UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\n",
    "    \"0\":\"\\u2764\\uFE0F\",\n",
    "    \"1\":\":baseball:\",\n",
    "    \"2\":\":grinning_face_with_big_eyes:\",\n",
    "    \"3\":\":disappointed_face:\",\n",
    "    \"4\":\":fork_and_knife:\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ù§Ô∏è\n",
      "‚öæ\n",
      "üòÉ\n",
      "üòû\n",
      "üç¥\n"
     ]
    }
   ],
   "source": [
    "for e in emoji_dictionary.values():\n",
    "    print(emoji.emojize(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_emoji.csv',header = None)\n",
    "test = pd.read_csv('test_emoji.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want to eat\\t</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he did not answer\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he got a raise\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she got me a present\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ha ha ha it was so funny\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0  1\n",
       "0             I want to eat\\t  4\n",
       "1         he did not answer\\t  3\n",
       "2            he got a raise\\t  2\n",
       "3      she got me a present\\t  0\n",
       "4  ha ha ha it was so funny\\t  2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>never talk to me again</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am proud of your achievements</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is the worst day in my life</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miss you so much</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>food is life</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0  1   2     3\n",
       "0           never talk to me again  3 NaN   NaN\n",
       "1  I am proud of your achievements  2 NaN   NaN\n",
       "2   It is the worst day in my life  3 NaN   NaN\n",
       "3                 Miss you so much  0 NaN   [0]\n",
       "4                     food is life  4 NaN   NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 4)\n"
     ]
    }
   ],
   "source": [
    "data = train.values\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[0]\n",
    "Y_train = train[1]\n",
    "\n",
    "X_test = test[0]\n",
    "Y_test = test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never talk to me again üòû\n",
      "I am proud of your achievements üòÉ\n",
      "It is the worst day in my life üòû\n",
      "Miss you so much ‚ù§Ô∏è\n",
      "food is life üç¥\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(X_train[i],emoji.emojize(emoji_dictionary[str(Y_train[i])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting sentences using Embeddings using Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Datasets/glove6b50dtxt/glove.6B.50d.txt',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "cnt=0\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float')\n",
    "    embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = embeddings_index['eat'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "print(emb_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converting sentence into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_output_train(X):\n",
    "    maxLen = 10\n",
    "    embedding_out = np.zeros((X.shape[0],maxLen,emb_dim))\n",
    "    \n",
    "    for ix in range(X.shape[0]):\n",
    "        X[ix] = X[ix].split()\n",
    "        for ij in range(len(X[ix])):\n",
    "            try:\n",
    "                embedding_out[ix][ij] = embeddings_index[X[ix][ij].lower()]\n",
    "            except:\n",
    "                embedding_out[ix][ij] = np.zeros((50,))\n",
    "    return embedding_out\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insanenerd/myvenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix_train = embedding_output_train(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insanenerd/myvenv/lib/python3.6/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "embeddings_matrix_test = embedding_output_train(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 10, 50) (132, 10, 50)\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_matrix_test.shape,embeddings_matrix_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132, 5)\n",
      "[0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "Y_train = to_categorical(Y_train,num_classes=5)\n",
    "Y_test = to_categorical(Y_test,num_classes=5)\n",
    "print(Y_train.shape)\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define RNN/LSTM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 29,765\n",
      "Trainable params: 29,765\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64,input_shape=(10,emb_dim)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 27 samples\n",
      "Epoch 1/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.6158 - accuracy: 0.2656\n",
      "Epoch 00001: val_loss improved from inf to 1.60425, saving model to best_model.h5\n",
      "105/105 [==============================] - 2s 16ms/sample - loss: 1.6101 - accuracy: 0.2476 - val_loss: 1.6043 - val_accuracy: 0.1852\n",
      "Epoch 2/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.5614 - accuracy: 0.3125\n",
      "Epoch 00002: val_loss improved from 1.60425 to 1.59984, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 438us/sample - loss: 1.5645 - accuracy: 0.3333 - val_loss: 1.5998 - val_accuracy: 0.1852\n",
      "Epoch 3/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.5533 - accuracy: 0.2969\n",
      "Epoch 00003: val_loss did not improve from 1.59984\n",
      "105/105 [==============================] - 0s 288us/sample - loss: 1.5547 - accuracy: 0.3048 - val_loss: 1.6004 - val_accuracy: 0.1481\n",
      "Epoch 4/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.5316 - accuracy: 0.4062\n",
      "Epoch 00004: val_loss did not improve from 1.59984\n",
      "105/105 [==============================] - 0s 354us/sample - loss: 1.5239 - accuracy: 0.3905 - val_loss: 1.6035 - val_accuracy: 0.1481\n",
      "Epoch 5/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.5156 - accuracy: 0.2969\n",
      "Epoch 00005: val_loss did not improve from 1.59984\n",
      "105/105 [==============================] - 0s 359us/sample - loss: 1.4989 - accuracy: 0.3619 - val_loss: 1.6075 - val_accuracy: 0.1481\n",
      "Epoch 6/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.5183 - accuracy: 0.3906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insanenerd/myvenv/lib/python3.6/site-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00006: val_loss did not improve from 1.59984\n",
      "105/105 [==============================] - 0s 384us/sample - loss: 1.4990 - accuracy: 0.4286 - val_loss: 1.6127 - val_accuracy: 0.1481\n",
      "Epoch 7/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.4679 - accuracy: 0.3750\n",
      "Epoch 00007: val_loss did not improve from 1.59984\n",
      "105/105 [==============================] - 0s 481us/sample - loss: 1.4746 - accuracy: 0.3810 - val_loss: 1.6194 - val_accuracy: 0.1481\n",
      "Epoch 8/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.4464 - accuracy: 0.4531\n",
      "Epoch 00008: val_loss did not improve from 1.59984\n",
      "105/105 [==============================] - 0s 349us/sample - loss: 1.4663 - accuracy: 0.4286 - val_loss: 1.6248 - val_accuracy: 0.1852\n",
      "Epoch 9/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.4337 - accuracy: 0.4062\n",
      "Epoch 00009: val_loss did not improve from 1.59984\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 1.4344 - accuracy: 0.4095 - val_loss: 1.6295 - val_accuracy: 0.1481\n",
      "Epoch 10/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.3930 - accuracy: 0.4219\n",
      "Epoch 00010: val_loss did not improve from 1.59984\n",
      "105/105 [==============================] - 0s 264us/sample - loss: 1.3817 - accuracy: 0.4381 - val_loss: 1.6357 - val_accuracy: 0.1481\n",
      "Epoch 11/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.3929 - accuracy: 0.4531\n",
      "Epoch 00011: val_loss did not improve from 1.59984\n",
      "105/105 [==============================] - 0s 279us/sample - loss: 1.3989 - accuracy: 0.4286 - val_loss: 1.6355 - val_accuracy: 0.1481\n",
      "Epoch 12/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.3755 - accuracy: 0.4062\n",
      "Epoch 00012: val_loss did not improve from 1.59984\n",
      "105/105 [==============================] - 0s 308us/sample - loss: 1.3922 - accuracy: 0.4095 - val_loss: 1.6204 - val_accuracy: 0.1852\n",
      "Epoch 13/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.3275 - accuracy: 0.5000\n",
      "Epoch 00013: val_loss improved from 1.59984 to 1.59453, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 568us/sample - loss: 1.3579 - accuracy: 0.4476 - val_loss: 1.5945 - val_accuracy: 0.1852\n",
      "Epoch 14/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.3922 - accuracy: 0.4375\n",
      "Epoch 00014: val_loss improved from 1.59453 to 1.55823, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 434us/sample - loss: 1.3358 - accuracy: 0.5143 - val_loss: 1.5582 - val_accuracy: 0.2222\n",
      "Epoch 15/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.3033 - accuracy: 0.5156\n",
      "Epoch 00015: val_loss improved from 1.55823 to 1.51942, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 433us/sample - loss: 1.3120 - accuracy: 0.5143 - val_loss: 1.5194 - val_accuracy: 0.2593\n",
      "Epoch 16/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.3115 - accuracy: 0.5469\n",
      "Epoch 00016: val_loss improved from 1.51942 to 1.48202, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 543us/sample - loss: 1.2613 - accuracy: 0.5905 - val_loss: 1.4820 - val_accuracy: 0.2593\n",
      "Epoch 17/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.2515 - accuracy: 0.5312\n",
      "Epoch 00017: val_loss improved from 1.48202 to 1.44638, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 515us/sample - loss: 1.2098 - accuracy: 0.5810 - val_loss: 1.4464 - val_accuracy: 0.1852\n",
      "Epoch 18/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.2370 - accuracy: 0.5625\n",
      "Epoch 00018: val_loss improved from 1.44638 to 1.40750, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 468us/sample - loss: 1.1741 - accuracy: 0.6190 - val_loss: 1.4075 - val_accuracy: 0.2222\n",
      "Epoch 19/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.1455 - accuracy: 0.6562\n",
      "Epoch 00019: val_loss improved from 1.40750 to 1.37589, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 610us/sample - loss: 1.1285 - accuracy: 0.6571 - val_loss: 1.3759 - val_accuracy: 0.2593\n",
      "Epoch 20/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.0342 - accuracy: 0.6719\n",
      "Epoch 00020: val_loss improved from 1.37589 to 1.34663, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 489us/sample - loss: 1.0746 - accuracy: 0.6476 - val_loss: 1.3466 - val_accuracy: 0.4074\n",
      "Epoch 21/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.9607 - accuracy: 0.6719\n",
      "Epoch 00021: val_loss improved from 1.34663 to 1.30399, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 485us/sample - loss: 1.0228 - accuracy: 0.6381 - val_loss: 1.3040 - val_accuracy: 0.4815\n",
      "Epoch 22/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.0013 - accuracy: 0.6406\n",
      "Epoch 00022: val_loss improved from 1.30399 to 1.25049, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 536us/sample - loss: 0.9740 - accuracy: 0.6381 - val_loss: 1.2505 - val_accuracy: 0.4815\n",
      "Epoch 23/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.8594 - accuracy: 0.7500\n",
      "Epoch 00023: val_loss improved from 1.25049 to 1.21324, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 403us/sample - loss: 0.8923 - accuracy: 0.7143 - val_loss: 1.2132 - val_accuracy: 0.5185\n",
      "Epoch 24/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 1.0369 - accuracy: 0.6094\n",
      "Epoch 00024: val_loss improved from 1.21324 to 1.17760, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 433us/sample - loss: 0.9201 - accuracy: 0.6952 - val_loss: 1.1776 - val_accuracy: 0.5185\n",
      "Epoch 25/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.8341 - accuracy: 0.7188\n",
      "Epoch 00025: val_loss improved from 1.17760 to 1.16461, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 471us/sample - loss: 0.8086 - accuracy: 0.7333 - val_loss: 1.1646 - val_accuracy: 0.4815\n",
      "Epoch 26/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.8124 - accuracy: 0.7188\n",
      "Epoch 00026: val_loss improved from 1.16461 to 1.15358, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 386us/sample - loss: 0.7658 - accuracy: 0.7429 - val_loss: 1.1536 - val_accuracy: 0.5185\n",
      "Epoch 27/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.8365 - accuracy: 0.7031\n",
      "Epoch 00027: val_loss improved from 1.15358 to 1.11714, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 437us/sample - loss: 0.7590 - accuracy: 0.7429 - val_loss: 1.1171 - val_accuracy: 0.5556\n",
      "Epoch 28/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.6657 - accuracy: 0.8125\n",
      "Epoch 00028: val_loss improved from 1.11714 to 1.06993, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 485us/sample - loss: 0.7175 - accuracy: 0.7714 - val_loss: 1.0699 - val_accuracy: 0.5556\n",
      "Epoch 29/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.7008 - accuracy: 0.7500\n",
      "Epoch 00029: val_loss improved from 1.06993 to 0.99759, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 515us/sample - loss: 0.6230 - accuracy: 0.8095 - val_loss: 0.9976 - val_accuracy: 0.5556\n",
      "Epoch 30/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.6539 - accuracy: 0.8281\n",
      "Epoch 00030: val_loss improved from 0.99759 to 0.95087, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 410us/sample - loss: 0.5891 - accuracy: 0.8190 - val_loss: 0.9509 - val_accuracy: 0.5926\n",
      "Epoch 31/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.6252 - accuracy: 0.8125\n",
      "Epoch 00031: val_loss did not improve from 0.95087\n",
      "105/105 [==============================] - 0s 243us/sample - loss: 0.5839 - accuracy: 0.8286 - val_loss: 0.9618 - val_accuracy: 0.6667\n",
      "Epoch 32/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.4488 - accuracy: 0.8594\n",
      "Epoch 00032: val_loss did not improve from 0.95087\n",
      "105/105 [==============================] - 0s 267us/sample - loss: 0.5280 - accuracy: 0.8286 - val_loss: 0.9768 - val_accuracy: 0.7037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.5579 - accuracy: 0.7812\n",
      "Epoch 00033: val_loss improved from 0.95087 to 0.93815, saving model to best_model.h5\n",
      "105/105 [==============================] - 0s 439us/sample - loss: 0.5014 - accuracy: 0.8381 - val_loss: 0.9381 - val_accuracy: 0.5926\n",
      "Epoch 34/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.5031 - accuracy: 0.8438\n",
      "Epoch 00034: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.4350 - accuracy: 0.8571 - val_loss: 0.9600 - val_accuracy: 0.5926\n",
      "Epoch 35/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.4448 - accuracy: 0.8438\n",
      "Epoch 00035: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.4261 - accuracy: 0.8762 - val_loss: 0.9986 - val_accuracy: 0.6296\n",
      "Epoch 36/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.3651 - accuracy: 0.8594\n",
      "Epoch 00036: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 292us/sample - loss: 0.3697 - accuracy: 0.8667 - val_loss: 1.1352 - val_accuracy: 0.6296\n",
      "Epoch 37/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.3206 - accuracy: 0.9062\n",
      "Epoch 00037: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 255us/sample - loss: 0.3570 - accuracy: 0.8952 - val_loss: 1.1770 - val_accuracy: 0.6296\n",
      "Epoch 38/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.3592 - accuracy: 0.8906\n",
      "Epoch 00038: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.3185 - accuracy: 0.9048 - val_loss: 1.0647 - val_accuracy: 0.5926\n",
      "Epoch 39/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.3554 - accuracy: 0.8906\n",
      "Epoch 00039: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 258us/sample - loss: 0.3258 - accuracy: 0.8857 - val_loss: 1.0757 - val_accuracy: 0.6667\n",
      "Epoch 40/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.2433 - accuracy: 0.9531\n",
      "Epoch 00040: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 281us/sample - loss: 0.2596 - accuracy: 0.9333 - val_loss: 1.2044 - val_accuracy: 0.5556\n",
      "Epoch 41/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.3217 - accuracy: 0.9062\n",
      "Epoch 00041: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.2889 - accuracy: 0.9333 - val_loss: 1.2139 - val_accuracy: 0.5926\n",
      "Epoch 42/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1792 - accuracy: 0.9531\n",
      "Epoch 00042: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 288us/sample - loss: 0.2310 - accuracy: 0.9429 - val_loss: 1.1714 - val_accuracy: 0.6296\n",
      "Epoch 43/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1878 - accuracy: 0.9375\n",
      "Epoch 00043: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 270us/sample - loss: 0.1902 - accuracy: 0.9524 - val_loss: 1.2014 - val_accuracy: 0.5556\n",
      "Epoch 44/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1727 - accuracy: 0.9688\n",
      "Epoch 00044: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 259us/sample - loss: 0.1779 - accuracy: 0.9429 - val_loss: 1.1638 - val_accuracy: 0.6667\n",
      "Epoch 45/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1956 - accuracy: 0.9531\n",
      "Epoch 00045: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 238us/sample - loss: 0.1801 - accuracy: 0.9524 - val_loss: 1.1747 - val_accuracy: 0.6667\n",
      "Epoch 46/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1193 - accuracy: 0.9844\n",
      "Epoch 00046: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 290us/sample - loss: 0.1325 - accuracy: 0.9714 - val_loss: 1.0917 - val_accuracy: 0.5926\n",
      "Epoch 47/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1630 - accuracy: 0.9531\n",
      "Epoch 00047: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 316us/sample - loss: 0.1247 - accuracy: 0.9714 - val_loss: 0.9974 - val_accuracy: 0.7037\n",
      "Epoch 48/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1496 - accuracy: 0.9844\n",
      "Epoch 00048: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 335us/sample - loss: 0.1389 - accuracy: 0.9810 - val_loss: 1.0061 - val_accuracy: 0.6296\n",
      "Epoch 49/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0606 - accuracy: 1.0000\n",
      "Epoch 00049: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 285us/sample - loss: 0.0940 - accuracy: 0.9905 - val_loss: 1.0457 - val_accuracy: 0.6667\n",
      "Epoch 50/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1209 - accuracy: 0.9688\n",
      "Epoch 00050: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 290us/sample - loss: 0.1318 - accuracy: 0.9619 - val_loss: 0.9714 - val_accuracy: 0.6296\n",
      "Epoch 51/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1013 - accuracy: 0.9844\n",
      "Epoch 00051: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 262us/sample - loss: 0.1042 - accuracy: 0.9810 - val_loss: 0.9941 - val_accuracy: 0.7037\n",
      "Epoch 52/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0638 - accuracy: 1.0000\n",
      "Epoch 00052: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.0773 - accuracy: 0.9905 - val_loss: 1.0544 - val_accuracy: 0.6667\n",
      "Epoch 53/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1013 - accuracy: 0.9688\n",
      "Epoch 00053: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 272us/sample - loss: 0.0911 - accuracy: 0.9810 - val_loss: 1.2087 - val_accuracy: 0.6667\n",
      "Epoch 54/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0645 - accuracy: 1.0000\n",
      "Epoch 00054: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 260us/sample - loss: 0.0820 - accuracy: 0.9905 - val_loss: 1.2155 - val_accuracy: 0.6296\n",
      "Epoch 55/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0662 - accuracy: 1.0000\n",
      "Epoch 00055: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 0.0711 - accuracy: 0.9905 - val_loss: 1.1513 - val_accuracy: 0.5926\n",
      "Epoch 56/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0544 - accuracy: 1.0000\n",
      "Epoch 00056: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.0668 - accuracy: 0.9905 - val_loss: 1.1445 - val_accuracy: 0.6296\n",
      "Epoch 57/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0514 - accuracy: 1.0000\n",
      "Epoch 00057: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.0697 - accuracy: 0.9905 - val_loss: 1.1763 - val_accuracy: 0.5556\n",
      "Epoch 58/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0406 - accuracy: 1.0000\n",
      "Epoch 00058: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 462us/sample - loss: 0.0559 - accuracy: 0.9810 - val_loss: 1.2279 - val_accuracy: 0.6667\n",
      "Epoch 59/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0611 - accuracy: 1.0000\n",
      "Epoch 00059: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.0748 - accuracy: 0.9905 - val_loss: 1.1187 - val_accuracy: 0.6667\n",
      "Epoch 60/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0604 - accuracy: 1.0000\n",
      "Epoch 00060: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 360us/sample - loss: 0.0617 - accuracy: 1.0000 - val_loss: 1.0410 - val_accuracy: 0.5926\n",
      "Epoch 61/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1475 - accuracy: 0.9375\n",
      "Epoch 00061: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.1090 - accuracy: 0.9524 - val_loss: 1.1620 - val_accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0531 - accuracy: 1.0000\n",
      "Epoch 00062: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 261us/sample - loss: 0.0678 - accuracy: 0.9905 - val_loss: 1.2692 - val_accuracy: 0.6296\n",
      "Epoch 63/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0876 - accuracy: 1.0000\n",
      "Epoch 00063: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 279us/sample - loss: 0.0977 - accuracy: 0.9810 - val_loss: 1.1987 - val_accuracy: 0.6667\n",
      "Epoch 64/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 00064: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 273us/sample - loss: 0.0394 - accuracy: 1.0000 - val_loss: 1.2010 - val_accuracy: 0.5926\n",
      "Epoch 65/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1160 - accuracy: 0.9531\n",
      "Epoch 00065: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 200us/sample - loss: 0.1011 - accuracy: 0.9619 - val_loss: 1.4639 - val_accuracy: 0.5926\n",
      "Epoch 66/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0740 - accuracy: 0.9844\n",
      "Epoch 00066: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 253us/sample - loss: 0.0936 - accuracy: 0.9810 - val_loss: 1.4367 - val_accuracy: 0.5556\n",
      "Epoch 67/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0482 - accuracy: 1.0000\n",
      "Epoch 00067: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 278us/sample - loss: 0.0603 - accuracy: 0.9905 - val_loss: 1.4478 - val_accuracy: 0.5926\n",
      "Epoch 68/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1549 - accuracy: 0.9531\n",
      "Epoch 00068: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 338us/sample - loss: 0.1148 - accuracy: 0.9714 - val_loss: 1.3785 - val_accuracy: 0.6667\n",
      "Epoch 69/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0555 - accuracy: 0.9844\n",
      "Epoch 00069: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 265us/sample - loss: 0.0490 - accuracy: 0.9905 - val_loss: 1.3411 - val_accuracy: 0.6667\n",
      "Epoch 70/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0608 - accuracy: 0.9844\n",
      "Epoch 00070: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.0630 - accuracy: 0.9905 - val_loss: 1.2723 - val_accuracy: 0.6296\n",
      "Epoch 71/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0475 - accuracy: 1.0000\n",
      "Epoch 00071: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 298us/sample - loss: 0.0451 - accuracy: 1.0000 - val_loss: 1.3197 - val_accuracy: 0.5185\n",
      "Epoch 72/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0623 - accuracy: 0.9844\n",
      "Epoch 00072: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 264us/sample - loss: 0.0466 - accuracy: 0.9905 - val_loss: 1.3104 - val_accuracy: 0.5556\n",
      "Epoch 73/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0561 - accuracy: 0.9844\n",
      "Epoch 00073: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.0461 - accuracy: 0.9905 - val_loss: 1.3195 - val_accuracy: 0.6667\n",
      "Epoch 74/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0306 - accuracy: 1.0000\n",
      "Epoch 00074: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 280us/sample - loss: 0.0323 - accuracy: 1.0000 - val_loss: 1.3350 - val_accuracy: 0.6667\n",
      "Epoch 75/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0257 - accuracy: 1.0000\n",
      "Epoch 00075: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 295us/sample - loss: 0.0328 - accuracy: 1.0000 - val_loss: 1.3312 - val_accuracy: 0.6667\n",
      "Epoch 76/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0450 - accuracy: 1.0000\n",
      "Epoch 00076: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.0410 - accuracy: 1.0000 - val_loss: 1.3304 - val_accuracy: 0.6667\n",
      "Epoch 77/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0287 - accuracy: 1.0000\n",
      "Epoch 00077: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 243us/sample - loss: 0.0264 - accuracy: 1.0000 - val_loss: 1.3331 - val_accuracy: 0.6667\n",
      "Epoch 78/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0349 - accuracy: 1.0000\n",
      "Epoch 00078: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.0293 - accuracy: 1.0000 - val_loss: 1.3361 - val_accuracy: 0.6296\n",
      "Epoch 79/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0225 - accuracy: 1.0000\n",
      "Epoch 00079: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 264us/sample - loss: 0.0200 - accuracy: 1.0000 - val_loss: 1.3339 - val_accuracy: 0.6296\n",
      "Epoch 80/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 00080: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 252us/sample - loss: 0.0226 - accuracy: 1.0000 - val_loss: 1.3243 - val_accuracy: 0.6296\n",
      "Epoch 81/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 00081: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 0.0153 - accuracy: 1.0000 - val_loss: 1.3134 - val_accuracy: 0.6296\n",
      "Epoch 82/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0264 - accuracy: 1.0000\n",
      "Epoch 00082: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.0251 - accuracy: 1.0000 - val_loss: 1.3167 - val_accuracy: 0.6296\n",
      "Epoch 83/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0546 - accuracy: 0.9844\n",
      "Epoch 00083: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 243us/sample - loss: 0.0380 - accuracy: 0.9905 - val_loss: 1.3123 - val_accuracy: 0.6296\n",
      "Epoch 84/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 00084: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 288us/sample - loss: 0.0187 - accuracy: 1.0000 - val_loss: 1.3115 - val_accuracy: 0.6296\n",
      "Epoch 85/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 00085: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.0191 - accuracy: 1.0000 - val_loss: 1.3170 - val_accuracy: 0.6296\n",
      "Epoch 86/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0306 - accuracy: 0.9844\n",
      "Epoch 00086: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 309us/sample - loss: 0.0240 - accuracy: 0.9905 - val_loss: 1.2935 - val_accuracy: 0.6296\n",
      "Epoch 87/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 00087: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.0147 - accuracy: 1.0000 - val_loss: 1.2786 - val_accuracy: 0.6296\n",
      "Epoch 88/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0335 - accuracy: 0.9844\n",
      "Epoch 00088: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 289us/sample - loss: 0.0288 - accuracy: 0.9905 - val_loss: 1.2772 - val_accuracy: 0.6296\n",
      "Epoch 89/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 00089: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.0206 - accuracy: 1.0000 - val_loss: 1.2749 - val_accuracy: 0.6296\n",
      "Epoch 90/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0372 - accuracy: 0.9844\n",
      "Epoch 00090: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 233us/sample - loss: 0.0287 - accuracy: 0.9905 - val_loss: 1.3128 - val_accuracy: 0.6296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 00091: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 259us/sample - loss: 0.0158 - accuracy: 1.0000 - val_loss: 1.3586 - val_accuracy: 0.6296\n",
      "Epoch 92/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 00092: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.0186 - accuracy: 1.0000 - val_loss: 1.3647 - val_accuracy: 0.6296\n",
      "Epoch 93/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 00093: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 248us/sample - loss: 0.0178 - accuracy: 1.0000 - val_loss: 1.3393 - val_accuracy: 0.6296\n",
      "Epoch 94/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0140 - accuracy: 1.0000\n",
      "Epoch 00094: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 204us/sample - loss: 0.0163 - accuracy: 1.0000 - val_loss: 1.3022 - val_accuracy: 0.6296\n",
      "Epoch 95/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 00095: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 261us/sample - loss: 0.0158 - accuracy: 1.0000 - val_loss: 1.2734 - val_accuracy: 0.6296\n",
      "Epoch 96/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 00096: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 254us/sample - loss: 0.0128 - accuracy: 1.0000 - val_loss: 1.2432 - val_accuracy: 0.6296\n",
      "Epoch 97/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 00097: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 247us/sample - loss: 0.0118 - accuracy: 1.0000 - val_loss: 1.2270 - val_accuracy: 0.6296\n",
      "Epoch 98/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 00098: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.0209 - accuracy: 1.0000 - val_loss: 1.2323 - val_accuracy: 0.6296\n",
      "Epoch 99/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 00099: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.0153 - accuracy: 1.0000 - val_loss: 1.2523 - val_accuracy: 0.6296\n",
      "Epoch 100/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 00100: val_loss did not improve from 0.93815\n",
      "105/105 [==============================] - 0s 260us/sample - loss: 0.0125 - accuracy: 1.0000 - val_loss: 1.2742 - val_accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"best_model.h5\",monitor='val_loss',verbose=True,save_best_only=True)\n",
    "earlystop = EarlyStopping(monitor='val_acc',patience=10)\n",
    "hist = model.fit(embeddings_matrix_train,Y_train,epochs=100,batch_size=64,shuffle=True,validation_split=0.2,callbacks=[earlystop,checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(embeddings_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 3 0 2 2 3 2 4 2 1 2 0 3 1 3 2 2 3 2 0 3 4 0 3 3 3 0 4 2 0 1 0 2 3 3 2\n",
      " 3 1 2 1 0 0 1 2 3 2 2 3 1 3 0 3 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 80us/sample - loss: 1.0245 - accuracy: 0.5893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.024472645350865, 0.58928573]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(embeddings_matrix_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 10, 64)            29440     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 62,789\n",
      "Trainable params: 62,789\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64,input_shape=(10,emb_dim),return_sequences=True))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(LSTM(64,input_shape=(10,emb_dim),return_sequences=False))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 27 samples\n",
      "Epoch 1/150\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "105/105 [==============================] - 2s 19ms/sample - loss: 1.6225 - accuracy: 0.1810 - val_loss: 1.5934 - val_accuracy: 0.2222\n",
      "Epoch 2/150\n",
      "105/105 [==============================] - 0s 312us/sample - loss: 1.5933 - accuracy: 0.2286 - val_loss: 1.5869 - val_accuracy: 0.2222\n",
      "Epoch 3/150\n",
      "105/105 [==============================] - 0s 364us/sample - loss: 1.5583 - accuracy: 0.3143 - val_loss: 1.5815 - val_accuracy: 0.2222\n",
      "Epoch 4/150\n",
      "105/105 [==============================] - 0s 346us/sample - loss: 1.5233 - accuracy: 0.3714 - val_loss: 1.5795 - val_accuracy: 0.2222\n",
      "Epoch 5/150\n",
      "105/105 [==============================] - 0s 307us/sample - loss: 1.4977 - accuracy: 0.2762 - val_loss: 1.5807 - val_accuracy: 0.2222\n",
      "Epoch 6/150\n",
      "105/105 [==============================] - 0s 268us/sample - loss: 1.4965 - accuracy: 0.3905 - val_loss: 1.5845 - val_accuracy: 0.2222\n",
      "Epoch 7/150\n",
      "105/105 [==============================] - 0s 316us/sample - loss: 1.4638 - accuracy: 0.3905 - val_loss: 1.5880 - val_accuracy: 0.2222\n",
      "Epoch 8/150\n",
      "105/105 [==============================] - 0s 302us/sample - loss: 1.4480 - accuracy: 0.3714 - val_loss: 1.5869 - val_accuracy: 0.2222\n",
      "Epoch 9/150\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 1.4018 - accuracy: 0.4095 - val_loss: 1.5826 - val_accuracy: 0.2593\n",
      "Epoch 10/150\n",
      "105/105 [==============================] - 0s 329us/sample - loss: 1.3792 - accuracy: 0.4190 - val_loss: 1.5653 - val_accuracy: 0.2593\n",
      "Epoch 11/150\n",
      "105/105 [==============================] - 0s 300us/sample - loss: 1.3409 - accuracy: 0.4857 - val_loss: 1.5420 - val_accuracy: 0.1852\n",
      "Epoch 12/150\n",
      "105/105 [==============================] - 0s 280us/sample - loss: 1.3108 - accuracy: 0.4667 - val_loss: 1.5014 - val_accuracy: 0.2222\n",
      "Epoch 13/150\n",
      "105/105 [==============================] - 0s 360us/sample - loss: 1.3099 - accuracy: 0.4571 - val_loss: 1.4392 - val_accuracy: 0.3333\n",
      "Epoch 14/150\n",
      "105/105 [==============================] - 0s 386us/sample - loss: 1.2089 - accuracy: 0.6000 - val_loss: 1.3717 - val_accuracy: 0.3704\n",
      "Epoch 15/150\n",
      "105/105 [==============================] - 0s 375us/sample - loss: 1.1728 - accuracy: 0.5905 - val_loss: 1.3015 - val_accuracy: 0.3704\n",
      "Epoch 16/150\n",
      "105/105 [==============================] - 0s 239us/sample - loss: 1.1304 - accuracy: 0.6190 - val_loss: 1.2372 - val_accuracy: 0.4074\n",
      "Epoch 17/150\n",
      "105/105 [==============================] - 0s 359us/sample - loss: 1.0911 - accuracy: 0.5810 - val_loss: 1.1893 - val_accuracy: 0.5185\n",
      "Epoch 18/150\n",
      "105/105 [==============================] - 0s 322us/sample - loss: 1.0296 - accuracy: 0.6190 - val_loss: 1.1585 - val_accuracy: 0.5556\n",
      "Epoch 19/150\n",
      "105/105 [==============================] - 0s 246us/sample - loss: 0.9734 - accuracy: 0.6571 - val_loss: 1.1406 - val_accuracy: 0.4444\n",
      "Epoch 20/150\n",
      "105/105 [==============================] - 0s 316us/sample - loss: 0.9335 - accuracy: 0.6476 - val_loss: 1.1198 - val_accuracy: 0.5185\n",
      "Epoch 21/150\n",
      "105/105 [==============================] - 0s 303us/sample - loss: 0.8770 - accuracy: 0.7048 - val_loss: 1.0980 - val_accuracy: 0.5185\n",
      "Epoch 22/150\n",
      "105/105 [==============================] - 0s 311us/sample - loss: 0.8279 - accuracy: 0.6857 - val_loss: 1.0614 - val_accuracy: 0.5185\n",
      "Epoch 23/150\n",
      "105/105 [==============================] - 0s 332us/sample - loss: 0.7820 - accuracy: 0.6952 - val_loss: 1.0912 - val_accuracy: 0.4815\n",
      "Epoch 24/150\n",
      "105/105 [==============================] - 0s 305us/sample - loss: 0.7681 - accuracy: 0.7429 - val_loss: 1.0676 - val_accuracy: 0.5185\n",
      "Epoch 25/150\n",
      "105/105 [==============================] - 0s 326us/sample - loss: 0.7514 - accuracy: 0.7143 - val_loss: 0.9925 - val_accuracy: 0.5556\n",
      "Epoch 26/150\n",
      "105/105 [==============================] - 0s 390us/sample - loss: 0.6271 - accuracy: 0.8476 - val_loss: 1.0440 - val_accuracy: 0.5556\n",
      "Epoch 27/150\n",
      "105/105 [==============================] - 0s 279us/sample - loss: 0.7181 - accuracy: 0.7905 - val_loss: 1.0718 - val_accuracy: 0.5185\n",
      "Epoch 28/150\n",
      "105/105 [==============================] - 0s 289us/sample - loss: 0.5853 - accuracy: 0.8095 - val_loss: 0.9736 - val_accuracy: 0.5556\n",
      "Epoch 29/150\n",
      "105/105 [==============================] - 0s 416us/sample - loss: 0.5547 - accuracy: 0.8381 - val_loss: 0.8783 - val_accuracy: 0.5556\n",
      "Epoch 30/150\n",
      "105/105 [==============================] - 0s 343us/sample - loss: 0.5241 - accuracy: 0.8667 - val_loss: 0.8172 - val_accuracy: 0.5926\n",
      "Epoch 31/150\n",
      "105/105 [==============================] - 0s 342us/sample - loss: 0.4776 - accuracy: 0.8476 - val_loss: 0.9482 - val_accuracy: 0.5926\n",
      "Epoch 32/150\n",
      "105/105 [==============================] - 0s 275us/sample - loss: 0.4578 - accuracy: 0.8571 - val_loss: 0.9989 - val_accuracy: 0.5556\n",
      "Epoch 33/150\n",
      "105/105 [==============================] - 0s 346us/sample - loss: 0.3952 - accuracy: 0.8857 - val_loss: 0.7927 - val_accuracy: 0.6296\n",
      "Epoch 34/150\n",
      "105/105 [==============================] - 0s 340us/sample - loss: 0.4305 - accuracy: 0.8667 - val_loss: 0.6973 - val_accuracy: 0.6667\n",
      "Epoch 35/150\n",
      "105/105 [==============================] - 0s 366us/sample - loss: 0.3702 - accuracy: 0.8857 - val_loss: 0.6830 - val_accuracy: 0.6667\n",
      "Epoch 36/150\n",
      "105/105 [==============================] - 0s 323us/sample - loss: 0.3269 - accuracy: 0.9048 - val_loss: 0.7342 - val_accuracy: 0.6667\n",
      "Epoch 37/150\n",
      "105/105 [==============================] - 0s 348us/sample - loss: 0.2981 - accuracy: 0.8952 - val_loss: 0.7497 - val_accuracy: 0.7037\n",
      "Epoch 38/150\n",
      "105/105 [==============================] - 0s 283us/sample - loss: 0.2762 - accuracy: 0.9143 - val_loss: 0.8204 - val_accuracy: 0.7037\n",
      "Epoch 39/150\n",
      "105/105 [==============================] - 0s 365us/sample - loss: 0.2375 - accuracy: 0.9333 - val_loss: 0.9827 - val_accuracy: 0.5926\n",
      "Epoch 40/150\n",
      "105/105 [==============================] - 0s 406us/sample - loss: 0.2282 - accuracy: 0.9619 - val_loss: 1.0202 - val_accuracy: 0.5926\n",
      "Epoch 41/150\n",
      "105/105 [==============================] - 0s 380us/sample - loss: 0.2575 - accuracy: 0.9143 - val_loss: 0.9267 - val_accuracy: 0.6667\n",
      "Epoch 42/150\n",
      "105/105 [==============================] - 0s 390us/sample - loss: 0.2414 - accuracy: 0.9333 - val_loss: 0.9461 - val_accuracy: 0.6667\n",
      "Epoch 43/150\n",
      "105/105 [==============================] - 0s 331us/sample - loss: 0.1621 - accuracy: 0.9524 - val_loss: 0.9014 - val_accuracy: 0.6296\n",
      "Epoch 44/150\n",
      "105/105 [==============================] - 0s 322us/sample - loss: 0.1471 - accuracy: 0.9905 - val_loss: 0.8118 - val_accuracy: 0.7407\n",
      "Epoch 45/150\n",
      "105/105 [==============================] - 0s 308us/sample - loss: 0.1694 - accuracy: 0.9524 - val_loss: 0.8169 - val_accuracy: 0.7037\n",
      "Epoch 46/150\n",
      "105/105 [==============================] - 0s 291us/sample - loss: 0.2056 - accuracy: 0.9619 - val_loss: 1.0348 - val_accuracy: 0.6296\n",
      "Epoch 47/150\n",
      "105/105 [==============================] - 0s 293us/sample - loss: 0.1898 - accuracy: 0.9333 - val_loss: 1.0093 - val_accuracy: 0.6667\n",
      "Epoch 48/150\n",
      "105/105 [==============================] - 0s 318us/sample - loss: 0.1266 - accuracy: 0.9619 - val_loss: 1.0476 - val_accuracy: 0.6667\n",
      "Epoch 49/150\n",
      "105/105 [==============================] - 0s 298us/sample - loss: 0.2106 - accuracy: 0.9619 - val_loss: 1.0859 - val_accuracy: 0.7037\n",
      "Epoch 50/150\n",
      "105/105 [==============================] - 0s 297us/sample - loss: 0.1537 - accuracy: 0.9524 - val_loss: 1.0896 - val_accuracy: 0.6296\n",
      "Epoch 51/150\n",
      "105/105 [==============================] - 0s 307us/sample - loss: 0.1189 - accuracy: 0.9714 - val_loss: 1.0799 - val_accuracy: 0.5926\n",
      "Epoch 52/150\n",
      "105/105 [==============================] - 0s 274us/sample - loss: 0.1517 - accuracy: 0.9429 - val_loss: 1.0783 - val_accuracy: 0.5926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/150\n",
      "105/105 [==============================] - 0s 321us/sample - loss: 0.0900 - accuracy: 0.9810 - val_loss: 0.9889 - val_accuracy: 0.6667\n",
      "Epoch 54/150\n",
      "105/105 [==============================] - 0s 278us/sample - loss: 0.1166 - accuracy: 0.9714 - val_loss: 1.1419 - val_accuracy: 0.6296\n",
      "Epoch 55/150\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.1136 - accuracy: 0.9810 - val_loss: 1.3727 - val_accuracy: 0.5556\n",
      "Epoch 56/150\n",
      "105/105 [==============================] - 0s 245us/sample - loss: 0.1027 - accuracy: 0.9714 - val_loss: 1.1428 - val_accuracy: 0.6667\n",
      "Epoch 57/150\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.1150 - accuracy: 0.9714 - val_loss: 1.2704 - val_accuracy: 0.6667\n",
      "Epoch 58/150\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.1984 - accuracy: 0.9619 - val_loss: 1.3285 - val_accuracy: 0.5926\n",
      "Epoch 59/150\n",
      "105/105 [==============================] - 0s 250us/sample - loss: 0.1023 - accuracy: 0.9810 - val_loss: 1.6972 - val_accuracy: 0.4815\n",
      "Epoch 60/150\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.1031 - accuracy: 0.9714 - val_loss: 1.7488 - val_accuracy: 0.4815\n",
      "Epoch 61/150\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.0573 - accuracy: 1.0000 - val_loss: 1.6691 - val_accuracy: 0.5185\n",
      "Epoch 62/150\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.0684 - accuracy: 0.9905 - val_loss: 1.5262 - val_accuracy: 0.5926\n",
      "Epoch 63/150\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 0.0549 - accuracy: 0.9905 - val_loss: 1.4947 - val_accuracy: 0.6296\n",
      "Epoch 64/150\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.0416 - accuracy: 1.0000 - val_loss: 1.6947 - val_accuracy: 0.5556\n",
      "Epoch 65/150\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.0436 - accuracy: 0.9905 - val_loss: 1.6968 - val_accuracy: 0.5556\n",
      "Epoch 66/150\n",
      "105/105 [==============================] - 0s 283us/sample - loss: 0.0451 - accuracy: 0.9905 - val_loss: 1.5530 - val_accuracy: 0.5926\n",
      "Epoch 67/150\n",
      "105/105 [==============================] - 0s 242us/sample - loss: 0.0760 - accuracy: 0.9810 - val_loss: 1.4836 - val_accuracy: 0.6667\n",
      "Epoch 68/150\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.0333 - accuracy: 1.0000 - val_loss: 1.4962 - val_accuracy: 0.5926\n",
      "Epoch 69/150\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 0.0337 - accuracy: 1.0000 - val_loss: 1.4597 - val_accuracy: 0.5926\n",
      "Epoch 70/150\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.0386 - accuracy: 1.0000 - val_loss: 1.3923 - val_accuracy: 0.6296\n",
      "Epoch 71/150\n",
      "105/105 [==============================] - 0s 223us/sample - loss: 0.0442 - accuracy: 0.9905 - val_loss: 1.3678 - val_accuracy: 0.5926\n",
      "Epoch 72/150\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.0406 - accuracy: 0.9905 - val_loss: 1.4548 - val_accuracy: 0.6296\n",
      "Epoch 73/150\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.0701 - accuracy: 0.9905 - val_loss: 1.4142 - val_accuracy: 0.6667\n",
      "Epoch 74/150\n",
      "105/105 [==============================] - 0s 224us/sample - loss: 0.0263 - accuracy: 1.0000 - val_loss: 1.4217 - val_accuracy: 0.6296\n",
      "Epoch 75/150\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.0498 - accuracy: 0.9905 - val_loss: 1.4660 - val_accuracy: 0.5926\n",
      "Epoch 76/150\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.0285 - accuracy: 1.0000 - val_loss: 1.5872 - val_accuracy: 0.5556\n",
      "Epoch 77/150\n",
      "105/105 [==============================] - 0s 226us/sample - loss: 0.0174 - accuracy: 1.0000 - val_loss: 1.7762 - val_accuracy: 0.5556\n",
      "Epoch 78/150\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 0.0456 - accuracy: 0.9905 - val_loss: 1.7885 - val_accuracy: 0.5556\n",
      "Epoch 79/150\n",
      "105/105 [==============================] - 0s 241us/sample - loss: 0.0244 - accuracy: 1.0000 - val_loss: 1.7689 - val_accuracy: 0.5556\n",
      "Epoch 80/150\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.0449 - accuracy: 0.9905 - val_loss: 1.5767 - val_accuracy: 0.5926\n",
      "Epoch 81/150\n",
      "105/105 [==============================] - 0s 228us/sample - loss: 0.0421 - accuracy: 0.9905 - val_loss: 1.5481 - val_accuracy: 0.5926\n",
      "Epoch 82/150\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.0224 - accuracy: 1.0000 - val_loss: 1.6315 - val_accuracy: 0.6667\n",
      "Epoch 83/150\n",
      "105/105 [==============================] - 0s 262us/sample - loss: 0.0260 - accuracy: 1.0000 - val_loss: 1.5727 - val_accuracy: 0.5926\n",
      "Epoch 84/150\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.0266 - accuracy: 1.0000 - val_loss: 1.6137 - val_accuracy: 0.5926\n",
      "Epoch 85/150\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.0261 - accuracy: 1.0000 - val_loss: 1.6198 - val_accuracy: 0.5556\n",
      "Epoch 86/150\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.0181 - accuracy: 1.0000 - val_loss: 1.5626 - val_accuracy: 0.5926\n",
      "Epoch 87/150\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.0244 - accuracy: 1.0000 - val_loss: 1.5400 - val_accuracy: 0.5556\n",
      "Epoch 88/150\n",
      "105/105 [==============================] - 0s 257us/sample - loss: 0.0288 - accuracy: 1.0000 - val_loss: 1.5490 - val_accuracy: 0.6667\n",
      "Epoch 89/150\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.0455 - accuracy: 0.9905 - val_loss: 1.4732 - val_accuracy: 0.5926\n",
      "Epoch 90/150\n",
      "105/105 [==============================] - 0s 219us/sample - loss: 0.0189 - accuracy: 1.0000 - val_loss: 1.5803 - val_accuracy: 0.6296\n",
      "Epoch 91/150\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.0324 - accuracy: 1.0000 - val_loss: 1.6305 - val_accuracy: 0.5556\n",
      "Epoch 92/150\n",
      "105/105 [==============================] - 0s 267us/sample - loss: 0.0218 - accuracy: 1.0000 - val_loss: 1.8826 - val_accuracy: 0.5185\n",
      "Epoch 93/150\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.0260 - accuracy: 1.0000 - val_loss: 1.8306 - val_accuracy: 0.5556\n",
      "Epoch 94/150\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.0345 - accuracy: 0.9905 - val_loss: 1.6455 - val_accuracy: 0.5556\n",
      "Epoch 95/150\n",
      "105/105 [==============================] - 0s 230us/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 1.4496 - val_accuracy: 0.7037\n",
      "Epoch 96/150\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.0467 - accuracy: 0.9810 - val_loss: 1.3847 - val_accuracy: 0.5926\n",
      "Epoch 97/150\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.0155 - accuracy: 1.0000 - val_loss: 1.5594 - val_accuracy: 0.5926\n",
      "Epoch 98/150\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.0374 - accuracy: 0.9810 - val_loss: 1.6089 - val_accuracy: 0.6296\n",
      "Epoch 99/150\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.0166 - accuracy: 1.0000 - val_loss: 1.5749 - val_accuracy: 0.6296\n",
      "Epoch 100/150\n",
      "105/105 [==============================] - 0s 235us/sample - loss: 0.0266 - accuracy: 1.0000 - val_loss: 1.5403 - val_accuracy: 0.5556\n",
      "Epoch 101/150\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.0196 - accuracy: 1.0000 - val_loss: 1.6194 - val_accuracy: 0.6667\n",
      "Epoch 102/150\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.0215 - accuracy: 0.9905 - val_loss: 1.6920 - val_accuracy: 0.6667\n",
      "Epoch 103/150\n",
      "105/105 [==============================] - 0s 231us/sample - loss: 0.0125 - accuracy: 1.0000 - val_loss: 1.7746 - val_accuracy: 0.6667\n",
      "Epoch 104/150\n",
      "105/105 [==============================] - 0s 227us/sample - loss: 0.0151 - accuracy: 1.0000 - val_loss: 1.8310 - val_accuracy: 0.6667\n",
      "Epoch 105/150\n",
      "105/105 [==============================] - 0s 220us/sample - loss: 0.0133 - accuracy: 1.0000 - val_loss: 1.8094 - val_accuracy: 0.6667\n",
      "Epoch 106/150\n",
      "105/105 [==============================] - 0s 271us/sample - loss: 0.0117 - accuracy: 1.0000 - val_loss: 1.7812 - val_accuracy: 0.6296\n",
      "Epoch 107/150\n",
      "105/105 [==============================] - 0s 266us/sample - loss: 0.0138 - accuracy: 1.0000 - val_loss: 1.7535 - val_accuracy: 0.6296\n",
      "Epoch 108/150\n",
      "105/105 [==============================] - 0s 245us/sample - loss: 0.0173 - accuracy: 1.0000 - val_loss: 1.7957 - val_accuracy: 0.6296\n",
      "Epoch 109/150\n",
      "105/105 [==============================] - 0s 236us/sample - loss: 0.0121 - accuracy: 1.0000 - val_loss: 1.8513 - val_accuracy: 0.6296\n",
      "Epoch 110/150\n",
      "105/105 [==============================] - 0s 221us/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 1.8875 - val_accuracy: 0.5926\n",
      "Epoch 111/150\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.0217 - accuracy: 1.0000 - val_loss: 1.8927 - val_accuracy: 0.5926\n",
      "Epoch 112/150\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.0080 - accuracy: 1.0000 - val_loss: 1.9092 - val_accuracy: 0.5926\n",
      "Epoch 113/150\n",
      "105/105 [==============================] - 0s 207us/sample - loss: 0.0106 - accuracy: 1.0000 - val_loss: 1.9372 - val_accuracy: 0.5926\n",
      "Epoch 114/150\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.0114 - accuracy: 1.0000 - val_loss: 1.9541 - val_accuracy: 0.5926\n",
      "Epoch 115/150\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.0144 - accuracy: 1.0000 - val_loss: 1.9746 - val_accuracy: 0.5926\n",
      "Epoch 116/150\n",
      "105/105 [==============================] - 0s 279us/sample - loss: 0.0177 - accuracy: 1.0000 - val_loss: 2.0055 - val_accuracy: 0.5926\n",
      "Epoch 117/150\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.0305 - val_accuracy: 0.5926\n",
      "Epoch 118/150\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.0156 - accuracy: 1.0000 - val_loss: 2.0372 - val_accuracy: 0.5926\n",
      "Epoch 119/150\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.0062 - accuracy: 1.0000 - val_loss: 2.0323 - val_accuracy: 0.5926\n",
      "Epoch 120/150\n",
      "105/105 [==============================] - 0s 205us/sample - loss: 0.0084 - accuracy: 1.0000 - val_loss: 2.0280 - val_accuracy: 0.6296\n",
      "Epoch 121/150\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.0085 - accuracy: 1.0000 - val_loss: 2.0283 - val_accuracy: 0.6296\n",
      "Epoch 122/150\n",
      "105/105 [==============================] - 0s 206us/sample - loss: 0.0139 - accuracy: 1.0000 - val_loss: 2.0080 - val_accuracy: 0.6296\n",
      "Epoch 123/150\n",
      "105/105 [==============================] - 0s 217us/sample - loss: 0.0073 - accuracy: 1.0000 - val_loss: 1.9873 - val_accuracy: 0.6296\n",
      "Epoch 124/150\n",
      "105/105 [==============================] - 0s 225us/sample - loss: 0.0098 - accuracy: 1.0000 - val_loss: 1.9293 - val_accuracy: 0.6296\n",
      "Epoch 125/150\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.0092 - accuracy: 1.0000 - val_loss: 1.8766 - val_accuracy: 0.5926\n",
      "Epoch 126/150\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.0094 - accuracy: 1.0000 - val_loss: 1.8365 - val_accuracy: 0.5926\n",
      "Epoch 127/150\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.0090 - accuracy: 1.0000 - val_loss: 1.8136 - val_accuracy: 0.5926\n",
      "Epoch 128/150\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 1.8102 - val_accuracy: 0.5926\n",
      "Epoch 129/150\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.0104 - accuracy: 1.0000 - val_loss: 1.8708 - val_accuracy: 0.5926\n",
      "Epoch 130/150\n",
      "105/105 [==============================] - 0s 234us/sample - loss: 0.0092 - accuracy: 1.0000 - val_loss: 1.9281 - val_accuracy: 0.5926\n",
      "Epoch 131/150\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.0057 - accuracy: 1.0000 - val_loss: 1.9738 - val_accuracy: 0.5926\n",
      "Epoch 132/150\n",
      "105/105 [==============================] - 0s 216us/sample - loss: 0.0065 - accuracy: 1.0000 - val_loss: 2.0156 - val_accuracy: 0.5926\n",
      "Epoch 133/150\n",
      "105/105 [==============================] - 0s 206us/sample - loss: 0.0118 - accuracy: 1.0000 - val_loss: 2.0523 - val_accuracy: 0.5926\n",
      "Epoch 134/150\n",
      "105/105 [==============================] - 0s 229us/sample - loss: 0.0103 - accuracy: 1.0000 - val_loss: 2.0794 - val_accuracy: 0.5926\n",
      "Epoch 135/150\n",
      "105/105 [==============================] - 0s 209us/sample - loss: 0.0091 - accuracy: 1.0000 - val_loss: 2.1105 - val_accuracy: 0.6296\n",
      "Epoch 136/150\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.0105 - accuracy: 1.0000 - val_loss: 2.1498 - val_accuracy: 0.5926\n",
      "Epoch 137/150\n",
      "105/105 [==============================] - 0s 215us/sample - loss: 0.0171 - accuracy: 0.9905 - val_loss: 2.1010 - val_accuracy: 0.6296\n",
      "Epoch 138/150\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.0067 - accuracy: 1.0000 - val_loss: 1.9461 - val_accuracy: 0.5926\n",
      "Epoch 139/150\n",
      "105/105 [==============================] - 0s 240us/sample - loss: 0.0117 - accuracy: 1.0000 - val_loss: 1.9791 - val_accuracy: 0.6667\n",
      "Epoch 140/150\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.0558 - accuracy: 0.9905 - val_loss: 2.1276 - val_accuracy: 0.6667\n",
      "Epoch 141/150\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.0070 - accuracy: 1.0000 - val_loss: 2.4395 - val_accuracy: 0.5556\n",
      "Epoch 142/150\n",
      "105/105 [==============================] - 0s 222us/sample - loss: 0.0177 - accuracy: 1.0000 - val_loss: 2.9572 - val_accuracy: 0.5185\n",
      "Epoch 143/150\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.0746 - accuracy: 0.9714 - val_loss: 2.1295 - val_accuracy: 0.6296\n",
      "Epoch 144/150\n",
      "105/105 [==============================] - 0s 218us/sample - loss: 0.0357 - accuracy: 0.9905 - val_loss: 1.9893 - val_accuracy: 0.6667\n",
      "Epoch 145/150\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.0613 - accuracy: 0.9810 - val_loss: 2.1293 - val_accuracy: 0.5185\n",
      "Epoch 146/150\n",
      "105/105 [==============================] - 0s 211us/sample - loss: 0.0082 - accuracy: 1.0000 - val_loss: 2.4377 - val_accuracy: 0.5185\n",
      "Epoch 147/150\n",
      "105/105 [==============================] - 0s 210us/sample - loss: 0.1804 - accuracy: 0.9714 - val_loss: 2.2122 - val_accuracy: 0.5185\n",
      "Epoch 148/150\n",
      "105/105 [==============================] - 0s 213us/sample - loss: 0.0301 - accuracy: 0.9905 - val_loss: 1.5668 - val_accuracy: 0.5556\n",
      "Epoch 149/150\n",
      "105/105 [==============================] - 0s 214us/sample - loss: 0.0083 - accuracy: 1.0000 - val_loss: 1.8085 - val_accuracy: 0.6296\n",
      "Epoch 150/150\n",
      "105/105 [==============================] - 0s 208us/sample - loss: 0.0917 - accuracy: 0.9619 - val_loss: 1.7077 - val_accuracy: 0.6296\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint(\"best_model1.h5\",monitor='val_loss',verbose=True,save_best_only=True)\n",
    "earlystop = EarlyStopping(monitor='val_acc',patience=10)\n",
    "hist = model.fit(embeddings_matrix_train,Y_train,epochs=150,batch_size=64,shuffle=True,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(\"best_model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(embeddings_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "print(len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 119us/sample - loss: 2.5910 - accuracy: 0.6071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.590971793447222, 0.60714287]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(embeddings_matrix_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to eat\n",
      "üç¥\n",
      "üç¥\n",
      "he did not answer\n",
      "üòû\n",
      "üòû\n",
      "he got a raise\n",
      "üòÉ\n",
      "üòû\n",
      "she got me a present\n",
      "‚ù§Ô∏è\n",
      "üòû\n",
      "ha ha ha it was so funny\n",
      "üòÉ\n",
      "üòÉ\n",
      "he is a good friend\n",
      "‚ù§Ô∏è\n",
      "üòÉ\n",
      "I am upset\n",
      "‚ù§Ô∏è\n",
      "üòû\n",
      "We had such a lovely dinner tonight\n",
      "‚ù§Ô∏è\n",
      "üòÉ\n",
      "where is the food\n",
      "üç¥\n",
      "üç¥\n",
      "Stop making this joke ha ha ha\n",
      "üòÉ\n",
      "üòÉ\n",
      "where is the ball\n",
      "‚öæ\n",
      "‚öæ\n",
      "work is hard\n",
      "üòû\n",
      "üòÉ\n",
      "This girl is messing with me\n",
      "üòû\n",
      "üòû\n",
      "are you serious ha ha\n",
      "üòÉ\n",
      "üòû\n",
      "Let us go play baseball\n",
      "‚öæ\n",
      "‚öæ\n",
      "This stupid grader is not working\n",
      "üòû\n",
      "üòû\n",
      "work is horrible\n",
      "üòû\n",
      "üòÉ\n",
      "Congratulation for having a baby\n",
      "üòÉ\n",
      "üòÉ\n",
      "stop messing around\n",
      "üòû\n",
      "üòû\n",
      "any suggestions for dinner\n",
      "üç¥\n",
      "üòÉ\n",
      "I love taking breaks\n",
      "‚ù§Ô∏è\n",
      "üòû\n",
      "you brighten my day\n",
      "üòÉ\n",
      "‚ù§Ô∏è\n",
      "I boiled rice\n",
      "üç¥\n",
      "üç¥\n",
      "she is a bully\n",
      "üòû\n",
      "üòÉ\n",
      "Why are you feeling bad\n",
      "üòû\n",
      "üòû\n",
      "I am upset\n",
      "üòû\n",
      "üòû\n",
      "I worked during my birthday\n",
      "üòû\n",
      "üòÉ\n",
      "My grandmother is the love of my life\n",
      "‚ù§Ô∏è\n",
      "‚ù§Ô∏è\n",
      "enjoy your break\n",
      "üòÉ\n",
      "‚öæ\n",
      "valentine day is near\n",
      "‚ù§Ô∏è\n",
      "üòÉ\n",
      "I miss you so much\n",
      "‚ù§Ô∏è\n",
      "‚ù§Ô∏è\n",
      "throw the ball\n",
      "‚öæ\n",
      "‚öæ\n",
      "My life is so boring\n",
      "üòû\n",
      "üòû\n",
      "she said yes\n",
      "üòÉ\n",
      "üòÉ\n",
      "will you be my valentine\n",
      "‚ù§Ô∏è\n",
      "‚ù§Ô∏è\n",
      "he can pitch really well\n",
      "‚öæ\n",
      "‚öæ\n",
      "dance with me\n",
      "üòÉ\n",
      "üòÉ\n",
      "I am starving\n",
      "üç¥\n",
      "üç¥\n",
      "See you at the restaurant\n",
      "üç¥\n",
      "üòÉ\n",
      "I like to laugh\n",
      "üòÉ\n",
      "üòÉ\n",
      "I will go dance\n",
      "üòÉ\n",
      "‚öæ\n",
      "I like your jacket\n",
      "üòÉ\n",
      "‚ù§Ô∏è\n",
      "i miss her\n",
      "‚ù§Ô∏è\n",
      "‚ù§Ô∏è\n",
      "what is your favorite baseball game\n",
      "‚öæ\n",
      "‚öæ\n",
      "Good job\n",
      "üòÉ\n",
      "üòÉ\n",
      "I love to the stars and back\n",
      "‚ù§Ô∏è\n",
      "üòÉ\n",
      "What you did was awesome\n",
      "üòÉ\n",
      "üòÉ\n",
      "ha ha ha lol\n",
      "üòÉ\n",
      "üòÉ\n",
      "I want to joke\n",
      "üòÉ\n",
      "üòû\n",
      "go away\n",
      "üòû\n",
      "üòû\n",
      "yesterday we lost again\n",
      "üòû\n",
      "üòû\n",
      "family is all I have\n",
      "‚ù§Ô∏è\n",
      "üòû\n",
      "you are failing this exercise\n",
      "üòû\n",
      "üòû\n",
      "Good joke\n",
      "üòÉ\n",
      "üòÉ\n",
      "You totally deserve this prize\n",
      "üòÉ\n",
      "üòû\n",
      "I did not have breakfast\n",
      "üòû\n",
      "üòû\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(pred)):\n",
    "    print(' '.join(X_test[i]))\n",
    "    print(emoji.emojize(emoji_dictionary[str(np.argmax(Y_test[i]))]))\n",
    "    print(emoji.emojize(emoji_dictionary[str(pred[i])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
